# Empathetic Chatbot: Inference Optimization & Benchmarking

This project explores high-performance inference techniques for Large Language Models (LLMs) in the context of an empathetic mental health chatbot.

It benchmarks `Llama-3-8B-Instruct` across four distinct inference strategies to analyze the trade-offs between Latency, Throughput, Memory Usage, and Response Quality.

Note: The author runs the following experiments on a single NVIDIA-L40 GPU environment.

## ðŸš€ Features & Modes
We implement four inference modes for comparison:
1. Baseline: Standard Hugging Face `FP16` inference.
2. Compiled: PyTorch 2.0 torch.compile() for kernel fusion and graph optimization.
3. Optimized: 4-bit Quantization (bitsandbytes) + Flash Attention 2 for low memory usage.
4. vLLM: A high-throughput serving engine using PagedAttention.

## Installation

### 1. Clone the Repository
```
git clone
cd empathetic_chatbot
```

### 2. Set up Environment
```
python -m venv .venv
source .venv/bin/activate  # On Windows: .venv\Scripts\activate
```

### 3. Install Dependencies
```
pip install -r requirements.txt
```
Optional: If your GPU supports Flash Attention 2 (Ampere A100/A10/RTX 30-series or newer), install the optimized kernel:

```
pip install flash-attn --no-build-isolation
```

### 4. Optional: Weights & Biases
```
wandb login
```

## Quick Start 
To run full benchmarking pipeline across all 4 modes sequentially:
```
python src/main.py --mode all --samples 50 --wandb
```
- `--samples 50`: Runs 50 empathetic prompts per model.
- `--wandb`: Logs metrics to Weights & Biases (remove flag to skip).
- Note: This command automatically handles GPU memory cleanup between runs.

## Evaluation & Plotting
After running the pipeline, generate the comparison charts (Quality, Latency, TTFT) and summary metrics:
```
python src/evaluate.py
```
Output:

- `results/quality_chart.png`: BERTScore comparison.

- `results/latency_chart.png`: End-to-end latency comparison.

- `results/ttft_chart.png`: Time-To-First-Token comparison.

- Terminal output with a summary table.

## Ablation studies
To run ablation studies:
```
python src/run_ablation.py
python evaluate_ablation.py
```

## Project Structure
```
.
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ main.py              # Main entry point for benchmarking
â”‚   â”œâ”€â”€ evaluate.py          # Generates charts and quality metrics
â”‚   â”œâ”€â”€ run_ablation.py      # Runs batch size scaling experiments
â”‚   â”œâ”€â”€ evaluate_ablation.py # Plots ablation results
â”‚   â”œâ”€â”€ config.py            # Global model configuration (prompts, params)
â”‚   â”œâ”€â”€ utils.py             # Shared utilities (metrics, logging, data 
loading)
â”‚   â””â”€â”€ modules/             # Optimization implementations
â”‚       â”œâ”€â”€ baseline.py      # Standard FP16 runner
â”‚       â”œâ”€â”€ compiled.py      # PyTorch 2.0 compiled runner
â”‚       â”œâ”€â”€ optimized.py     # 4-bit + Flash Attention runner
â”‚       â””â”€â”€ vllm_runner.py   # vLLM engine runner
â”œâ”€â”€ results/                 # Stores .csv data and .png charts
â”œâ”€â”€ requirements.txt         # Project dependencies
â””â”€â”€ README.md                # Project documentation
```